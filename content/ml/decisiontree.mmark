---
title: "面向面试的机器学习笔记——决策树1"
date: 2022-02-22T20:23:45+08:00
hidehome: false
draft: false
tags: ["ML","决策树"]
categories: "机器学习"
series: ["面向面试的机器学习笔记"]
---
本篇内容面向面试，主要记录【决策树】算法相关的细节。
<!--more-->
## 树的停止分裂条件

**停止分裂的条件**

  停止分裂的条件已经在**决策树**中阐述，这里不再进行阐述。

（1）最小节点数

当节点的==数据量小于一个指定的数量时==，不继续分裂。两个原因：一是数据量较少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。

（2）熵或者基尼值小于阀值。

由上述可知，熵和基尼值的大小表示数据的复杂程度，==当熵或者基尼值过小时，表示数据的纯度比较大==，如果熵或者基尼值小于一定程度时，节点停止分裂。

（3）决策树的深度达到指定的条件

节点的深度可以理解为节点与决策树跟节点的距离，如根节点的子节点的深度为1，因为这些节点与跟节点的距离为1，子节点的深度要比父节点的深度大1。决策树的深度是所有叶子节点的最大深度，==当深度到达指定的上限大小时，停止分裂。==

（4）==所有特征已经使用完毕，不能继续进行分裂。被动式停止分裂的条件==。

## 如何处理连续特征？

离散化。取所有不同值的中点作为划分点，计算信息增益。如C4.5中处理连续值，通常使用二分法。

## ID3缺陷

1. 无法处理连续值
2. 没有考虑缺失值
3. 信息增益会偏向于取值较多的特征
4. 无剪枝策略，较容易过拟合

## 决策树如何剪枝

参考资料：[决策树的后剪枝算法](https://www.cnblogs.com/starfire86/p/5749334.html)

### 先剪枝

也称为预剪枝，在构建树的同时进行剪枝操作，通常会预设超参作为剪枝依据。

通常的预剪枝指标：

* 树的最大深度
* 节点最小样本量
* 最小信息增益
* 其他满足树的生长停止的限制条件

优点：避免不必要的树的生长，节省算力和时间

缺点：容易欠拟合

### 后剪枝

指构建完树，通过某种条件遍历树从而进行剪枝操作，主要有如下方法：

* 降低错误剪枝 REP(Reduced Error Pruning)
* 悲观错误剪枝 PEP(Pessimistic Error Pruning)
* 基于错误剪枝 EBP(Error Based Pruning)
* 代价-复杂度剪枝 CCP(Cost Complexity Pruning)
* 最小错误剪枝 MEP(Minimum Error Pruning)

#### 悲观错误方法PEP——C4.5

Pessimistic Error Pruning

在C4.5算法中提出，相比REP不需要一个单独的测试集。在决策树分裂中，分裂后的子树相比分裂前在训练集上一定产生增益，但这种增益很有可能是过拟合带来的，在测试集上可能不会有这种增益，甚至带来错误。因此给予一个**悲观**的错误估计，用以下算式计算子树的错误率：

$$
Error = \frac {\sum E + 0.5 * L} { \sum N}
$$

其中E代表每一个叶子结点的错误样本数，**同时对每一个叶子结点施加一个0.5的错误惩罚**，来计算整个子树的错误样本数，再除以整个子树的样本数量，即可得到整个子树的错误率。因为悲观错误因子的加入，所以子树的误判率可能并未占到优势。根据误判率，我们可以认为一个子树对于任意样本划分正确与否的概率服从伯努力分布，那么如果样本为N，可以计算出子树误判次数的均值与标准差：

$$
\begin{align*}
 E(subtree\_err\_count) &= N*Error \\ 
 var(subtree\_err\_count) &= \sqrt {N*Error*(1-Error)}
\end{align*}
$$

把子树替换成叶子结点后，叶子结点的误判也服从伯努利分布。

$$
E(leaf\_err\_count) = N*e
$$

剪枝标准：

$$
E(subtree\_err\_count)-var(subtree\_err\_count)>E(leaf\_err\_count)
$$

![image.png](https://b3logfile.com/siyuan/1617901637880/assets/image-20220221205448-gxtz45z.png)

##### 缺点

1. 自上而下，容易剪枝过度
2. 剪枝失败（？）

#### 最小错误方法REP

以预留测试集的方式剪枝，步骤如下：

1. 删除以结点x为根的子树，使x根节点变为对应决策树的叶子结点
2. 赋予x结点关联训练数据中最常见的分类类别
3. 运用测试集进行测试，如果剪枝后的性能不比剪枝前差，则剪枝

#### 代价复杂度方法CCP——CART

定义代价与复杂度，代价指剪枝过程中增加的错分样本，复杂度表示剪枝后子树t减少的叶结点个数；

$$
\alpha=\frac{R(剪枝后叶子结点)-R(未剪枝的子树)}{|N_1|-1}
$$

其中$R=r(t)*p(t)$，r为错分样本旅，p为落入该结点的样本占所有样本的比率。

CCP的步骤：

1. 计算决策树中每一个非叶结点的$\alpha$值，循环剪掉最小的$\alpha$值（表示增益最小）的子树，直到剩下根节点。可得到一系列的决策树${T_0,T_1,...,T_m}$；
2. 从决策树序列中选择根据真实误差估计最优的决策树

![image.png](https://b3logfile.com/siyuan/1617901637880/assets/image-20220221211051-wdji76q.png)  
<br />

#### 后剪枝算法比较

|算法|REP|PEP|CCP|
| ----------| ----------------| ----------| ----------|
|剪枝顺序|自底向上|自顶向下|自底向上|
|复杂度|O(N)|O(N)|O(N2)|
|误差估计|测试集误差估计|连续纠正|标准误差|

## 决策树生成算法如何处理缺失值

结论先行：

* ID3算法中无法自动对缺失值处理
* C4.5算法用概率权重的方式处理缺失值
* CART用Surrogate Splits(替代划分)的方法处理缺失值

### C4.5样本概率权重

简单来说，C4.5在计算样本信息增益时，对于存在缺失的特征，会在该特征所有不缺失的样本上计算信息增益，再对未缺失计算得到的信息增益乘以缺失率。

![image.png](https://b3logfile.com/siyuan/1617901637880/assets/image-20220222195432-6ull2ld.png)

如果经过降权的特征再比较时仍然是信息增益最大的特征，那么会对缺失样本加权后进入到所有子节点中，例：如果最终分裂后存在特征缺失样本，并产生三个子节点，对应的样本数分别为5，7，3；那么这两个样本分别以权重5/15、7/15、3/15进入到三个叶子结点中。

### CART缺失值处理

缺失值的处理前期与C4.5相同，会在最终计算的信息增益上乘以非缺失率作为惩罚。

如果惩罚后的信息增益在所有特征中仍然是最大的：

Step1: 在其他的所有没有缺失值的特征中，选择信息增益与该特征最接近的进行分裂；

Step2: 如果设定了一定的分裂标准，导致没有特征与该缺失特征的信息增益差值满足要求，那么久对该缺失特征进行分裂，并把缺失的样本默认划分进入个数最大的叶子结点

（CART缺失值处理的运算量很大，在后续xgb，lgb的缺失处理中，默认将缺失值划分到信息增益最大的节点中）

## 决策树预测中如何处理缺失值

1. 如果有专门的缺失值处理分枝，那么可以走该分支；
2. 走训练时叶子结点中数量最多的分支；
3. 同时走所有的分支，最终得到多个结果，即不同类别的概率

## 参考资料

[决策树缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)

[各类决策树算法对缺失值的处理](https://zhuanlan.zhihu.com/p/84519568)
